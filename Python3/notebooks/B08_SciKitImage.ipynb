{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:red;background-color:black\">\n",
    "Diamond Light Source\n",
    "\n",
    "<h1 style=\"color:red;background-color:antiquewhite\"> Python Libraries: SciKitImage</h1>  \n",
    "\n",
    "Â©2000-20 Chris Seddon \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "Execute the following cell to activate styling for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import HTML\n",
    "HTML(f\"<style>{open('my.css').read()}</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "SciKitImage is a library that performs a number of image processing transformations on images.  All images, once read into memory are stored as Numpy arrays.  Images can be full color or monochrome.\n",
    "\n",
    "To get the feel of what an image looks like in memory, consider the following Numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(dpi=72*2)\n",
    "\n",
    "data = np.array(\n",
    "      [[ 25,  30,  35,  40,  45,  50,  55,  60,  65,  70,  75,  80,  85,  90,  95, 100, 105, 110, 115, 120],\n",
    "       [ 30,  36,  42,  48,  54,  60,  66,  72,  78,  84,  90,  96, 102, 108, 114, 120, 126, 132, 138, 144],\n",
    "       [ 35,  42,  49,  56,  63,  70,  77,  84,  91,  98, 105, 112, 119, 126, 133, 140, 147, 154, 161, 168],\n",
    "       [ 40,  48,  56,  64,  72,  80,  88,  96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192],\n",
    "       [ 45,  54,  63,  72,  81,  90,  99, 108, 117, 126, 135, 144, 153, 162, 171, 180, 189, 198, 207, 216],\n",
    "       [ 50,  60,  70,  80,  90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240],\n",
    "       [ 55,  66,  77,  88,  99, 110, 121, 132, 143, 154, 165, 176, 187, 198, 209, 220, 231, 242, 253,   8],\n",
    "       [ 60,  72,  84,  96, 108, 120, 132, 144, 156, 168, 180, 192, 204, 216, 228, 240, 252,   8,  20,  32],\n",
    "       [ 65,  78,  91, 104, 117, 130, 143, 156, 169, 182, 195, 208, 221, 234, 247,   4,  17,  30,  43,  56],\n",
    "       [ 70,  84,  98, 112, 126, 140, 154, 168, 182, 196, 210, 224, 238, 252,  10,  24,  38,  52,  66,  80],\n",
    "       [ 75,  90, 105, 120, 135, 150, 165, 180, 195, 210, 225, 240, 255,  14,  29,  44,  59,  74,  89, 104],\n",
    "       [ 80,  96, 112, 128, 144, 160, 176, 192, 208, 224, 240,   0,  16,  32,  48,  64,  80,  96, 112, 128],\n",
    "       [ 85, 102, 119, 136, 153, 170, 187, 204, 221, 238, 255,  16,  33,  50,  67,  84, 101, 118, 135, 152],\n",
    "       [ 90, 108, 126, 144, 162, 180, 198, 216, 234, 252,  14,  32,  50,  68,  86, 104, 122, 140, 158, 176],\n",
    "       [ 95, 114, 133, 152, 171, 190, 209, 228, 247,  10,  29,  48,  67,  86, 105, 124, 143, 162, 181, 200],\n",
    "       [100, 120, 140, 160, 180, 200, 220, 240,   4,  24,  44,  64,  84, 104, 124, 144, 164, 184, 204, 224],\n",
    "       [105, 126, 147, 168, 189, 210, 231, 252,  17,  38,  59,  80, 101, 122, 143, 164, 185, 206, 227, 248],\n",
    "       [110, 132, 154, 176, 198, 220, 242,   8,  30,  52,  74,  96, 118, 140, 162, 184, 206, 228, 250,  16],\n",
    "       [115, 138, 161, 184, 207, 230, 253,  20,  43,  66,  89, 112, 135, 158, 181, 204, 227, 250,  17,  40],\n",
    "       [120, 144, 168, 192, 216, 240,   8,  32,  56,  80, 104, 128, 152, 176, 200, 224, 248,  16,  40,  64]])\n",
    "plt.imshow(data, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "We've use MatPlotLib to display the image.  Note that MatPlotLib always uses a colormap when displayinh images and this gives false colors unless we use the \"gray\" colormap.  \n",
    "\n",
    "As you can see, the pixels have a value between 0 and 255 to represent color, with:\n",
    "* black is 0\n",
    "* white is 255\n",
    "\n",
    "Images can also be represented as floats between 0.0 and 1.0.  Here is the same image using floats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(dpi=72*2)\n",
    "\n",
    "data = np.array(\n",
    "      [[0.1 , 0.12, 0.14, 0.16, 0.18, 0.2 , 0.21, 0.23, 0.25, 0.27, 0.29, 0.31, 0.33, 0.35, 0.37, 0.39, 0.41, 0.43, 0.45, 0.47],\n",
    "       [0.12, 0.14, 0.16, 0.19, 0.21, 0.23, 0.26, 0.28, 0.3 , 0.33, 0.35, 0.38, 0.4 , 0.42, 0.45, 0.47, 0.49, 0.52, 0.54, 0.56],\n",
    "       [0.14, 0.16, 0.19, 0.22, 0.25, 0.27, 0.3 , 0.33, 0.36, 0.38, 0.41, 0.44, 0.46, 0.49, 0.52, 0.55, 0.57, 0.6 , 0.63, 0.66],\n",
    "       [0.16, 0.19, 0.22, 0.25, 0.28, 0.31, 0.34, 0.38, 0.41, 0.44, 0.47, 0.5 , 0.53, 0.56, 0.59, 0.62, 0.66, 0.69, 0.72, 0.75],\n",
    "       [0.18, 0.21, 0.25, 0.28, 0.32, 0.35, 0.39, 0.42, 0.46, 0.49, 0.53, 0.56, 0.6 , 0.63, 0.67, 0.7 , 0.74, 0.77, 0.81, 0.84],\n",
    "       [0.2 , 0.23, 0.27, 0.31, 0.35, 0.39, 0.43, 0.47, 0.51, 0.55, 0.59, 0.62, 0.66, 0.7 , 0.74, 0.78, 0.82, 0.86, 0.9 , 0.94],\n",
    "       [0.21, 0.26, 0.3 , 0.34, 0.39, 0.43, 0.47, 0.52, 0.56, 0.6 , 0.64, 0.69, 0.73, 0.77, 0.82, 0.86, 0.9 , 0.95, 0.99, 0.03],\n",
    "       [0.23, 0.28, 0.33, 0.38, 0.42, 0.47, 0.52, 0.56, 0.61, 0.66, 0.7 , 0.75, 0.8 , 0.84, 0.89, 0.94, 0.98, 0.03, 0.08, 0.12],\n",
    "       [0.25, 0.3 , 0.36, 0.41, 0.46, 0.51, 0.56, 0.61, 0.66, 0.71, 0.76, 0.81, 0.86, 0.91, 0.96, 0.02, 0.07, 0.12, 0.17, 0.22],\n",
    "       [0.27, 0.33, 0.38, 0.44, 0.49, 0.55, 0.6 , 0.66, 0.71, 0.77, 0.82, 0.88, 0.93, 0.98, 0.04, 0.09, 0.15, 0.2 , 0.26, 0.31],\n",
    "       [0.29, 0.35, 0.41, 0.47, 0.53, 0.59, 0.64, 0.7 , 0.76, 0.82, 0.88, 0.94, 1.  , 0.05, 0.11, 0.17, 0.23, 0.29, 0.35, 0.41],\n",
    "       [0.31, 0.38, 0.44, 0.5 , 0.56, 0.62, 0.69, 0.75, 0.81, 0.88, 0.94, 0.  , 0.06, 0.12, 0.19, 0.25, 0.31, 0.38, 0.44, 0.5 ],\n",
    "       [0.33, 0.4 , 0.46, 0.53, 0.6 , 0.66, 0.73, 0.8 , 0.86, 0.93, 1.  , 0.06, 0.13, 0.2 , 0.26, 0.33, 0.39, 0.46, 0.53, 0.59],\n",
    "       [0.35, 0.42, 0.49, 0.56, 0.63, 0.7 , 0.77, 0.84, 0.91, 0.98, 0.05, 0.12, 0.2 , 0.27, 0.34, 0.41, 0.48, 0.55, 0.62, 0.69],\n",
    "       [0.37, 0.45, 0.52, 0.59, 0.67, 0.74, 0.82, 0.89, 0.96, 0.04, 0.11, 0.19, 0.26, 0.34, 0.41, 0.48, 0.56, 0.63, 0.71, 0.78],\n",
    "       [0.39, 0.47, 0.55, 0.62, 0.7 , 0.78, 0.86, 0.94, 0.02, 0.09, 0.17, 0.25, 0.33, 0.41, 0.48, 0.56, 0.64, 0.72, 0.8 , 0.88],\n",
    "       [0.41, 0.49, 0.57, 0.66, 0.74, 0.82, 0.9 , 0.98, 0.07, 0.15, 0.23, 0.31, 0.39, 0.48, 0.56, 0.64, 0.72, 0.8 , 0.89, 0.97],\n",
    "       [0.43, 0.52, 0.6 , 0.69, 0.77, 0.86, 0.95, 0.03, 0.12, 0.2 , 0.29, 0.38, 0.46, 0.55, 0.63, 0.72, 0.8 , 0.89, 0.98, 0.06],\n",
    "       [0.45, 0.54, 0.63, 0.72, 0.81, 0.9 , 0.99, 0.08, 0.17, 0.26, 0.35, 0.44, 0.53, 0.62, 0.71, 0.8 , 0.89, 0.98, 0.07, 0.16],\n",
    "       [0.47, 0.56, 0.66, 0.75, 0.84, 0.94, 0.03, 0.12, 0.22, 0.31, 0.41, 0.5 , 0.59, 0.69, 0.78, 0.88, 0.97, 0.06, 0.16, 0.25]])\n",
    "plt.imshow(data, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "Color images are more complicated.  In this case each pixel has a value for the red, green and blue component.  Again values are in the range 0 to 255 or 0.0 to 1.0.\n",
    "\n",
    "Here is a 10 x 10 color array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(dpi=72*2)\n",
    "\n",
    "data = np.array(\n",
    "[\n",
    "    [[ 25, 50, 75],[ 30, 60, 90],[ 35, 70,105],[ 40, 80,120],[ 45, 90,135],[ 50,100,150],[ 55,110,165],[ 60,120,180],[ 65,130,195],[ 70,140,210]],\n",
    "    [[ 30, 60, 90],[ 36, 72,108],[ 42, 84,126],[ 48, 96,144],[ 54,108,162],[ 60,120,180],[ 66,132,198],[ 72,144,216],[ 78,156,234],[ 84,168,252]],\n",
    "    [[ 35, 70,105],[ 42, 84,126],[ 49, 98,147],[ 56,112,168],[ 63,126,189],[ 70,140,210],[ 77,154,231],[ 84,168,252],[ 91,182, 17],[ 98,196, 38]],\n",
    "    [[ 40, 80,120],[ 48, 96,144],[ 56,112,168],[ 64,128,192],[ 72,144,216],[ 80,160,240],[ 88,176,  8],[ 96,192, 32],[104,208, 56],[112,224, 80]],\n",
    "    [[ 45, 90,135],[ 54,108,162],[ 63,126,189],[ 72,144,216],[ 81,162,243],[ 90,180, 14],[ 99,198, 41],[108,216, 68],[117,234, 95],[126,252,122]],\n",
    "    [[ 50,100,150],[ 60,120,180],[ 70,140,210],[ 80,160,240],[ 90,180, 14],[100,200, 44],[110,220, 74],[120,240,104],[130,  4,134],[140, 24,164]],\n",
    "    [[ 55,110,165],[ 66,132,198],[ 77,154,231],[ 88,176,  8],[ 99,198, 41],[110,220, 74],[121,242,107],[132,  8,140],[143, 30,173],[154, 52,206]],\n",
    "    [[ 60,120,180],[ 72,144,216],[ 84,168,252],[ 96,192, 32],[108,216, 68],[120,240,104],[132,  8,140],[144, 32,176],[156, 56,212],[168, 80,248]],\n",
    "    [[ 65,130,195],[ 78,156,234],[ 91,182, 17],[104,208, 56],[117,234, 95],[130,  4,134],[143, 30,173],[156, 56,212],[169, 82,251],[182,108, 34]],\n",
    "    [[ 70,140,210],[ 84,168,252],[ 98,196, 38],[112,224, 80],[126,252,122],[140, 24,164],[154, 52,206],[168, 80,248],[182,108, 34],[196,136, 76]],\n",
    "])\n",
    "plt.imshow(data, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "Color images can also contain a transparency factor.  This is added to the RGB value for each pixel.  In this example I've set it to 200 for the top left pixels and 100 everywhere else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(dpi=72*2)\n",
    "\n",
    "data = np.array(\n",
    "[\n",
    "    [[ 25, 50, 75,200],[ 30, 60, 90,200],[ 35, 70,105,200],[ 40, 80,120,200],[ 45, 90,135,200],[ 50,100,150,200],[ 55,110,165,100],[ 60,120,180,100],[ 65,130,195,100],[ 70,140,210,100]],\n",
    "    [[ 30, 60, 90,200],[ 36, 72,108,200],[ 42, 84,126,200],[ 48, 96,144,200],[ 54,108,162,200],[ 60,120,180,200],[ 66,132,198,100],[ 72,144,216,100],[ 78,156,234,100],[ 84,168,252,100]],\n",
    "    [[ 35, 70,105,200],[ 42, 84,126,200],[ 49, 98,147,200],[ 56,112,168,200],[ 63,126,189,200],[ 70,140,210,200],[ 77,154,231,100],[ 84,168,252,100],[ 91,182, 17,100],[ 98,196, 38,100]],\n",
    "    [[ 40, 80,120,200],[ 48, 96,144,200],[ 56,112,168,200],[ 64,128,192,200],[ 72,144,216,200],[ 80,160,240,200],[ 88,176,  8,100],[ 96,192, 32,100],[104,208, 56,100],[112,224, 80,100]],\n",
    "    [[ 45, 90,135,200],[ 54,108,162,200],[ 63,126,189,200],[ 72,144,216,200],[ 81,162,243,200],[ 90,180, 14,200],[ 99,198, 41,100],[108,216, 68,100],[117,234, 95,100],[126,252,122,100]],\n",
    "    [[ 50,100,150,200],[ 60,120,180,200],[ 70,140,210,200],[ 80,160,240,200],[ 90,180, 14,200],[100,200, 44,200],[110,220, 74,100],[120,240,104,100],[130,  4,134,100],[140, 24,164,100]],\n",
    "    [[ 55,110,165,100],[ 66,132,198,100],[ 77,154,231,100],[ 88,176,  8,100],[ 99,198, 41,100],[110,220, 74,100],[121,242,107,100],[132,  8,140,100],[143, 30,173,100],[154, 52,206,100]],\n",
    "    [[ 60,120,180,100],[ 72,144,216,100],[ 84,168,252,100],[ 96,192, 32,100],[108,216, 68,100],[120,240,104,100],[132,  8,140,100],[144, 32,176,100],[156, 56,212,100],[168, 80,248,100]],\n",
    "    [[ 65,130,195,100],[ 78,156,234,100],[ 91,182, 17,100],[104,208, 56,100],[117,234, 95,100],[130,  4,134,100],[143, 30,173,100],[156, 56,212,100],[169, 82,251,100],[182,108, 34,100]],\n",
    "    [[ 70,140,210,100],[ 84,168,252,100],[ 98,196, 38,100],[112,224, 80,100],[126,252,122,100],[140, 24,164,100],[154, 52,206,100],[168, 80,248,100],[182,108, 34,100],[196,136, 76,100]],\n",
    "])\n",
    "plt.imshow(data, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\n",
    "Now lets work with a real monochrome image of rice grains.  We'll use PIL to read the image into memory and then convert to Numpy array of int:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(dpi=72*2)\n",
    "\n",
    "image = PIL.Image.open(\"images/rice.jpg\")\n",
    "rice = np.asarray(image, dtype=\"int32\")\n",
    "plt.imshow(rice, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7\n",
    "The idea is to count the number of grains of rice in this image.  \n",
    "\n",
    "Although the rice image looks monochrome it is actually a color image with the red, green and blue values equal for any particular pixel.  Let's print the shape of the Numpy array and look at a slice of the image to confirm this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"image shape = {rice.shape}\")\n",
    "print(f\"slice of image:\\n{rice[30:35,30:35]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8\n",
    "SciKitImage works with true monochrome images.  In our case we can simply choose one of the three color channels to form our monochrome image.  It doesn't matter which one as they are all identical.  I'll choose the red channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monochrome = rice[:,:,0]\n",
    "print(f\"image shape = {monochrome.shape}\")\n",
    "print(f\"slice of image:\\n{monochrome[30:35,30:35]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9\n",
    "To simplify the analysis we will convert the image into black and white.  What we have to do is look at each pixel in turn and decide if it is above a threshold.  If it is then change it to white, but otherwise change it to black.\n",
    "\n",
    "After playing around with the image, I found 120 was a good value to use as the threshold.  We can use Numpy's indexing ability to perform the transformation to black and white:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(dpi=72*2)\n",
    "\n",
    "def toBlackAndWhite(image, threshold):\n",
    "    image[ image[:,:] <= threshold ] = 0\n",
    "    image[ image[:,:]  > threshold ] = 255\n",
    "toBlackAndWhite(monochrome, 120)\n",
    "plt.imshow(monochrome, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10\n",
    "You'll notice this generates some speckle.  SciKitImage has an algorithm to remove it, but we have to \"label\" the image before we can use any SciKitImage algorithms.\n",
    "\n",
    "To see what \"label\" does we'll look at part of the image after the speckle is removed and inspect the underlying Numpy array.  Note that:<pre>morphology.remove_small_objects(labelled)</pre>\n",
    "corrupts the labelling, so we have to reapply labelling after calling \"remove_small_objects\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.measure as measure\n",
    "import skimage.morphology as morphology\n",
    "plt.subplots(dpi=72*2)\n",
    "\n",
    "labelled = measure.label(monochrome)\n",
    "labelled = morphology.remove_small_objects(labelled)\n",
    "#reapply labelling\n",
    "labelled = measure.label(labelled)\n",
    "\n",
    "colorMap = \"Set3\" \n",
    "plt.imshow(labelled[40:80,:30], cmap=colorMap)\n",
    "plt.show()\n",
    "print(labelled[40:80,:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11\n",
    "So labelling uses the white pixels to identify grains of rice.  In the part of the image shown above, we are seeing a small part of rice grain 14 and most of grain 17.  \n",
    "\n",
    "The colors displayed in the plot are dependent of the colormap \"cool\".  Of course the labelling extends across the entire array and we can now determine how many rice grains there are by looking at the largest value in the array: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.measure as measure\n",
    "import skimage.morphology as morphology\n",
    "plt.subplots(dpi=72*2)\n",
    "\n",
    "print(f\"Number of grains of rice = {np.amax(labelled)}\")\n",
    "plt.imshow(labelled, cmap=colorMap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12\n",
    "The labelling not only identifies each grain of rice, but by counting the pixels of a given label we can determine its size.  SciKitImage will do this for us using:\n",
    "<pre>skimage.measure.regionprops(labelled)</pre>\n",
    "While calculating the size of each grain, SciKitImage also calculates the position of the surrounding rectangle and its centroid.  \n",
    "\n",
    "We can add all this information to the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.measure as measure\n",
    "import skimage.morphology as morphology\n",
    "plt.subplots(dpi=72*2)\n",
    "\n",
    "props = measure.regionprops(labelled)\n",
    "for item in props:\n",
    "    y = item.centroid[0]\n",
    "    x = item.centroid[1]\n",
    "    message = str(item.label)\n",
    "    plt.text(x, y, message, color=\"black\")\n",
    "\n",
    "plt.imshow(labelled, cmap=colorMap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13\n",
    "Rice grain 85 is an interesting object.  Let's look more closely at that part of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.measure as measure\n",
    "import skimage.morphology as morphology\n",
    "import scipy.ndimage as nd\n",
    "\n",
    "plt.subplots(dpi=72*2)\n",
    "top = 350\n",
    "bottom = 450\n",
    "left = 100\n",
    "right = 200\n",
    "props = measure.regionprops(labelled)\n",
    "for item in props:\n",
    "    y = item.centroid[0] - top\n",
    "    x = item.centroid[1] - left\n",
    "    if (0 < x < right-left) and (0 < y < bottom-top):\n",
    "        message = str(item.label)\n",
    "        plt.text(x, y, message, color=\"black\")\n",
    "\n",
    "plt.imshow(labelled[top:bottom, left:right], cmap=colorMap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14\n",
    "Grain 85 has a hole in it.  We can use:<pre>scipy.ndimage.binary_fill_holes</pre>to fill in holes like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.measure as measure\n",
    "import skimage.morphology as morphology\n",
    "import scipy.ndimage as nd\n",
    "\n",
    "plt.subplots(dpi=72*2)\n",
    "labelled = nd.binary_fill_holes(labelled).astype(int)\n",
    "#reapply labelling\n",
    "labelled = measure.label(labelled)\n",
    "\n",
    "top = 350\n",
    "bottom = 450\n",
    "left = 100\n",
    "right = 200\n",
    "props = measure.regionprops(labelled)\n",
    "\n",
    "for item in props:\n",
    "    y = item.centroid[0] - top\n",
    "    x = item.centroid[1] - left\n",
    "    if (0 < x < right-left) and (0 < y < bottom-top):\n",
    "        message = str(item.label)\n",
    "        plt.text(x, y, message, color=\"black\")\n",
    "\n",
    "plt.imshow(labelled[top:bottom, left:right], cmap=colorMap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15\n",
    "SciKitImage has many other algorithms that can be applied to different image types.  \n",
    "One of the more interesting algorithms is edge detection.  The following edge detection algorithm was developed by John F. Canny in 1986.\n",
    "\n",
    "For starters we need an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "# import PIL.Image as Image\n",
    "# import PIL.ImageDraw as ImageDraw\n",
    "\n",
    "plt.subplots(dpi=72*2)\n",
    "image = PIL.Image.open(\"images/chris.jpg\")\n",
    "# grid = Image.new('RGBA', size=(image.width, image.height), color=(255,255,255))\n",
    "# blank = Image.new('RGBA', size=(image.width, image.height), color=(255,255,255))\n",
    "# step_size = 50\n",
    "\n",
    "# gridCanvas = ImageDraw.Draw(grid)\n",
    "# for x in range(0, image.width, step_size):\n",
    "#     line = ((x, 0), (x, image.height))\n",
    "#     gridCanvas.line(line, fill=(0, 0, 0), width=1)\n",
    "    \n",
    "# for y in range(0, image.height, step_size):\n",
    "#     line = ((0, y), (image.width, y))\n",
    "#     gridCanvas.line(line, fill=(0, 0, 0), width=1)\n",
    "\n",
    "# Make sure images got an alpha channel\n",
    "#image = image.convert(\"RGBA\")\n",
    "plt.imshow(image, cmap=plt.cm.gray) #    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16\n",
    "This is obviously a color image, but our algorithm requires a monochrome image.  Let's use PIL to perform the conversion (it takes a weighted averaged of the RGB values): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(dpi=72*2)\n",
    "\n",
    "image = PIL.Image.open(\"images/chris.jpg\")\n",
    "\n",
    "image = image.convert('L')\n",
    "image = np.asarray(image, dtype=\"int32\")\n",
    "plt.imshow(image, cmap=plt.cm.gray) #    plt.show()\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17\n",
    "According to the docs:\n",
    "\n",
    "\"The Canny filter is a multi-stage edge detector. It uses a filter based on the derivative of a Gaussian in order to compute the intensity of the gradients.The Gaussian reduces the effect of noise present in the image. Then, potential edges are thinned down to 1-pixel curves by removing non-maximum pixels of the gradient magnitude. Finally, edge pixels are kept or removed using hysteresis thresholding on the gradient magnitude.\n",
    "\n",
    "The Canny has three adjustable parameters: the width of the Gaussian (the noisier the image, the greater the width), and the low and high threshold for the hysteresis thresholding.\"\n",
    "\n",
    "Now let's apply the Canny algorithm.  The algorithm depends on the 3 parameters:\n",
    "\n",
    "* sigma\n",
    "* low_threshold\n",
    "* high_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.feature\n",
    "plt.subplots(dpi=72*2)\n",
    "\n",
    "Ï = 1\n",
    "edges = skimage.feature.canny(image, sigma=Ï, low_threshold=10, high_threshold=20)\n",
    "plt.imshow(edges, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18\n",
    "Looks as though we've made a bad choice of these parameters.  After some trial and error I came up with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.feature\n",
    "plt.subplots(dpi=72*2)\n",
    "\n",
    "Ï = 2\n",
    "edges = skimage.feature.canny(image, sigma=Ï, low_threshold=27, high_threshold=30)\n",
    "plt.imshow(edges, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19\n",
    "Much better!\n",
    "\n",
    "The Canny edge detection is also used with other algorithms such as the Hough transformation.\n",
    "\n",
    "The Hough transformation was originally developed to detect straight lines, but can also be used with other shapes.  Originally invented for machine analysis of bubble chamber photographs (Hough, 1959) the algorithm was enhanced by Richard Duda and Peter Hart in 1972.  \n",
    "\n",
    "We'll use the Hough transformation to detect tablets in the following photograph.  The transformation is applied after edge detection (in our case using Canny):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplots(dpi=72*2)\n",
    "\n",
    "tablets = PIL.Image.open(\"images/tablets.jpg\")\n",
    "plt.imshow(tablets)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20\n",
    "At this point it would be a good idea to investigate details of the Hough transformation (please google it).\n",
    "\n",
    "Although we've been using the PIL library to load images into memory, SciKitImage has a build in function to do this; furthermore, it can convert a color image into greyscale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io \n",
    "plt.subplots(dpi=72*2)\n",
    "\n",
    "tablets = io.imread(\"images/tablets.jpg\", as_gray=True)\n",
    "plt.imshow(tablets, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21\n",
    "As discussed above, we need to use an edge detection algorithm before we can apply Hough.  Note that <pre>skimage.io.imread</pre>\n",
    "returns an array of floats.  We can multiply by 256 to convert to the equivalent int values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "from skimage.feature import canny\n",
    "\n",
    "plt.subplots(dpi=72*2)\n",
    "\n",
    "# perform edge detection on grayscal image\n",
    "image = io.imread(\"images/tablets.jpg\", as_gray=True) * 256\n",
    "edges = canny(image, sigma=4, low_threshold=0, high_threshold=30)\n",
    "plt.imshow(edges, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22\n",
    "The Hough transformation relies on us specifying suitable values for the circle radii.  After some trial and error I found suitable tablet radii of 33, 34 and 35.\n",
    "\n",
    "We now perform the Hough transformation and obtain accumulators for each pixel in the image using these radii with the method:<pre>hough_circle</pre>\n",
    "\n",
    "Once we have the results for all pixels, we can select the pixels with the highest votes using the method:<pre>hough_circle_peaks</pre>\n",
    "\n",
    "This method returns the accumulators with the most votes and the coordinates of the corresponding circles with their radii.  We are not really interested in the votes, but we can create a Pandas dataframe from the rest of the information.  \n",
    "\n",
    "I found I had to search for the top 200 peaks; any less and we miss tablets.  Unfortunately, the results contain a lot a \"near\" duplicates - circles that differ by one or two pixels.  We'll have to fix that later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "from skimage.feature import canny\n",
    "from skimage.transform import hough_circle, hough_circle_peaks\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# perform edge detection on grayscal image\n",
    "image = io.imread(\"images/tablets.jpg\", as_gray=True) * 256\n",
    "edges = canny(image, sigma=4, low_threshold=0, high_threshold=30)\n",
    "\n",
    "# radii range from 33 to 35\n",
    "tablet_radii = [33, 34, 35]\n",
    "\n",
    "# perform Hough transform on image\n",
    "accumulators_for_every_pixel = hough_circle(edges, tablet_radii)\n",
    "\n",
    "# select the most prominent circles\n",
    "top_accumulators, cx, cy, radii = hough_circle_peaks(accumulators_for_every_pixel, tablet_radii, total_num_peaks=200)\n",
    "\n",
    "# create a dataframe with the results\n",
    "df = pd.DataFrame(zip(cx, cy, radii))\n",
    "df.columns = ['cx', 'cy', 'r']\n",
    "df = df.sort_values(by=['cx', 'cy'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23\n",
    "Examining the above we can see all the \"near\" duplicates.  For example, the rows with index 5, 6, 7, 8 and 9 all represent the same circle.\n",
    "\n",
    "We can remove these duplicates by iterating through the dataframe and eliminating rows if their \"cx\" and \"cy\" coordinates are close to an existing entry.  This involves a double iteration through the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNearDuplicates(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "    delta = 15\n",
    "    duplicates = []\n",
    "    for rowA in df.itertuples(name='A'):\n",
    "        da = rowA._asdict()\n",
    "        for rowB in df.itertuples(name='B'):\n",
    "            db = rowB._asdict()\n",
    "            if db['Index'] <= da['Index']: continue\n",
    "            if abs(da['cx'] - db['cx']) < delta and abs(da['cy'] - db['cy']) < delta:\n",
    "                duplicates.append(db['Index'])\n",
    "                \n",
    "    # the above algorithm may detect a duplicate more than once; use set to correct\n",
    "    for d in set(duplicates):\n",
    "        df = df.drop(d)\n",
    "    \n",
    "    # drop=True discards the old index\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df = removeNearDuplicates(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24\n",
    "Now we can plot the tablets with their ordinal number (Index+1).  Alas, one tablet gets missed.\n",
    "\n",
    "For completeness, I'll give you all the code for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "from skimage.transform import hough_circle, hough_circle_peaks\n",
    "from skimage.feature import canny\n",
    "from skimage.draw import circle_perimeter\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "def removeNearDuplicates(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "    delta = 15\n",
    "    duplicates = []\n",
    "    for rowA in df.itertuples(name='A'):\n",
    "        da = rowA._asdict()\n",
    "        for rowB in df.itertuples(name='B'):\n",
    "            db = rowB._asdict()\n",
    "            if db['Index'] <= da['Index']: continue\n",
    "            if abs(da['cx'] - db['cx']) < delta and abs(da['cy'] - db['cy']) < delta:\n",
    "                duplicates.append(db['Index'])\n",
    "                \n",
    "    # the above algorithm may detect a duplicate more than once; use set to correct\n",
    "    for d in set(duplicates):\n",
    "        df = df.drop(d)\n",
    "    \n",
    "    # drop=True discards the old index\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# perform edge detection on grayscal image\n",
    "image = io.imread(\"images/tablets.jpg\", as_gray=True) * 256\n",
    "edges = canny(image, sigma=4, low_threshold=0, high_threshold=30)\n",
    "\n",
    "# radii range from 33 to 35\n",
    "tablet_radii = [33, 34, 35]\n",
    "\n",
    "# perform Hough transform on image\n",
    "accumulators_for_every_pixel = hough_circle(edges, tablet_radii)\n",
    "\n",
    "# select the most prominent circles\n",
    "top_accumulators, cx, cy, radii = hough_circle_peaks(accumulators_for_every_pixel, tablet_radii, total_num_peaks=200)\n",
    "\n",
    "# form dataframe so we can remove \"near\" duplicates\n",
    "df = pd.DataFrame(zip(cx, cy, radii))\n",
    "df.columns = ['cx', 'cy', 'r']\n",
    "df = df.sort_values(by=['cx', 'cy'])\n",
    "df = removeNearDuplicates(df)\n",
    "       \n",
    "# display the original image with the circles highlighted\n",
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(10, 4), dpi=72*2)\n",
    "image = io.imread(\"images/tablets.jpg\", as_gray=False)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # determine points on circle perimeters\n",
    "    cy, cx = circle_perimeter(row['cy'], row['cx'], row['r'], shape=image.shape)\n",
    "    # print ordinal number in eacg circle\n",
    "    plt.text(row['cx']-18, row['cy']+9, f\"{index+1:2}\", color=\"red\", fontsize=6)\n",
    "    # highlight circle primeters (linewidth = 5 pixels)\n",
    "    r = [-2,-1,0,1,2]\n",
    "    for dx in r:\n",
    "        for dy in r:\n",
    "            image[cy+dy, cx+dx] = (0, 255, 0)    \n",
    "ax.imshow(image, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
